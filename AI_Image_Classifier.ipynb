

!pip install gradio
     
Collecting gradio
  Downloading gradio-4.36.1-py3-none-any.whl (12.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 41.3 MB/s eta 0:00:00
Collecting aiofiles<24.0,>=22.0 (from gradio)
  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)
Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)
Collecting fastapi (from gradio)
  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 10.0 MB/s eta 0:00:00
Collecting ffmpy (from gradio)
  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)
  Preparing metadata (setup.py) ... done
Collecting gradio-client==1.0.1 (from gradio)
  Downloading gradio_client-1.0.1-py3-none-any.whl (318 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.1/318.1 kB 26.4 MB/s eta 0:00:00
Collecting httpx>=0.24.1 (from gradio)
  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 8.1 MB/s eta 0:00:00
Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.3)
Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)
Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)
Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)
Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)
Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)
Collecting orjson~=3.0 (from gradio)
  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145.0/145.0 kB 10.8 MB/s eta 0:00:00
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)
Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)
Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)
Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.3)
Collecting pydub (from gradio)
  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)
Collecting python-multipart>=0.0.9 (from gradio)
  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)
Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)
Collecting ruff>=0.2.2 (from gradio)
  Downloading ruff-0.4.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 75.6 MB/s eta 0:00:00
Collecting semantic-version~=2.0 (from gradio)
  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)
Collecting tomlkit==0.12.0 (from gradio)
  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)
Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)
Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)
Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)
Collecting uvicorn>=0.14.0 (from gradio)
  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 7.3 MB/s eta 0:00:00
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)
Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio)
  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 16.9 MB/s eta 0:00:00
Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)
Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)
Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)
Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)
Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)
Collecting httpcore==1.* (from httpx>=0.24.1->gradio)
  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 9.6 MB/s eta 0:00:00
Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)
Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)
Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)
  Downloading h11-0.14.0-py3-none-any.whl (58 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.0 MB/s eta 0:00:00
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)
Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)
Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)
Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)
Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)
Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)
Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)
Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)
Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)
Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)
Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)
Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)
  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 8.7 MB/s eta 0:00:00
Collecting fastapi-cli>=0.0.2 (from fastapi->gradio)
  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)
Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)
  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 kB 6.8 MB/s eta 0:00:00
Collecting email_validator>=2.0.0 (from fastapi->gradio)
  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)
Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)
  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 32.2 MB/s eta 0:00:00
Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)
Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)
Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)
Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)
  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 34.9 MB/s eta 0:00:00
Collecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)
  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)
Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)
  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 83.4 MB/s eta 0:00:00
Collecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)
  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 49.0 MB/s eta 0:00:00
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)
Building wheels for collected packages: ffmpy
  Building wheel for ffmpy (setup.py) ... done
  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=5459eb52d84a432c8f4d28972246eef8896216af73a9dd7f436977fa9de35179
  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81
Successfully built ffmpy
Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, fastapi, gradio
Successfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.36.1 gradio-client-1.0.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.5 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.9 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import random
import plotly.express as px
from scipy import ndimage
from shutil import copyfile
import cv2
from sklearn.model_selection import train_test_split
from skimage.io import imread
from skimage.transform import resize
from tensorflow.keras.layers import Conv2D, Add, MaxPooling2D, Dense, BatchNormalization, Input, Flatten, Dropout, LeakyReLU, AveragePooling2D
from tensorflow.keras.models import Model, Sequential, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import gradio as gr
     

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'cifake-real-and-ai-generated-synthetic-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3041726%2F5256696%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240614%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240614T153038Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5ca84c2f6739bd1f97d5742fc6661e2ff0e37df986a6be661d91b1af6909cdc67c117f9d273cac1637e187d4553ad44b0205d8f586b49f596fee408608708dcc9d9d36f2fddbd7d6659391dbbe90b3c0b1a48f45b78bfc25137a4d5daa05d52932f1a753f08ef282833ece4132ddb0fb5786b9a41430f53fe2a2e7d410728a6229cd4b0036ef852acd43ff614f703c756aca912be8254bddf4fa2d480ebe7fa47993b09364fc26f8dac492263b0fcf13c90639d6ac1e042b050d8e69696c95963c2e329af1acddc6f96109ca4c0a3895c53d4f11b8516c85735b85445c67b837b3cfe5a85085a5b74856837794a60e910b3266150b7d2e7a1e173cbc7f1bedc4'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

     
Downloading cifake-real-and-ai-generated-synthetic-images, 109625224 bytes compressed
[==================================================] 109625224 bytes downloaded
Downloaded and uncompressed: cifake-real-and-ai-generated-synthetic-images
Data source import complete.

def create_directory(directory_path):
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)

def sample_and_copy_images(src_directory, dest_directory, sample_fraction=0.25):
    create_directory(dest_directory)
    for sub_dir in ['FAKE', 'REAL']:
        src_path = os.path.join(src_directory, sub_dir)
        dest_path = os.path.join(dest_directory, sub_dir)
        create_directory(dest_path)

        # Get list of all files in the source directory
        all_files = os.listdir(src_path)

        # Calculate number of samples to take
        num_samples = int(len(all_files) * sample_fraction)

        # Randomly sample files
        sampled_files = random.sample(all_files, num_samples)

        # Copy sampled files to destination directory
        for file_name in sampled_files:
            shutil.copy(os.path.join(src_path, file_name), os.path.join(dest_path, file_name))

# Define source and destination directories
src_train_directory = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train'
src_test_directory = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test'

dest_directory = '/kaggle/working/sampled_data'

# Sample and copy images for train and test datasets
sample_and_copy_images(src_train_directory, os.path.join(dest_directory, 'train'), sample_fraction=0.25)
sample_and_copy_images(src_test_directory, os.path.join(dest_directory, 'test'), sample_fraction=0.25)

print(f"Sampled images have been copied to {dest_directory}")
     
Sampled images have been copied to /kaggle/working/sampled_data

def load_image_paths_and_labels(directory):
    data = []
    labels = []
    for label, sub_dir in enumerate(['FAKE', 'REAL']):
        path = os.path.join(directory, sub_dir)
        for file_name in os.listdir(path):
            file_path = os.path.join(path, file_name)
            data.append(file_path)
            labels.append('FAKE' if label == 0 else 'REAL')
    return data, labels

# Load train and test image paths and labels
train_paths, train_labels = load_image_paths_and_labels(os.path.join(dest_directory, 'train'))
test_paths, test_labels = load_image_paths_and_labels(os.path.join(dest_directory, 'test'))

# Split the training data into training and validation sets (80% train, 20% validation)
train_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size=0.2, random_state=42, stratify=train_labels)
     

def image_whitening(image):
    mean = np.mean(image)
    std = np.std(image)
    whitened_image = (image - mean) / std
    return whitened_image

def image_normalization(image):
    normalized_image = image / 255.0
    return normalized_image
     

# Create an instance of the ImageDataGenerator with preprocessing functions
datagen = ImageDataGenerator(preprocessing_function=lambda x: image_whitening(image_normalization(x)))

# data generators
train_generator = datagen.flow_from_dataframe(
    dataframe=pd.DataFrame({'filename': train_paths, 'class': train_labels}),
    x_col='filename',
    y_col='class',
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

valid_generator = datagen.flow_from_dataframe(
    dataframe=pd.DataFrame({'filename': valid_paths, 'class': valid_labels}),
    x_col='filename',
    y_col='class',
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

test_generator = datagen.flow_from_dataframe(
    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels}),
    x_col='filename',
    y_col='class',
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)
     
Found 20000 validated image filenames belonging to 2 classes.
Found 5000 validated image filenames belonging to 2 classes.
Found 5000 validated image filenames belonging to 2 classes.

# Define the CNN model
model = Sequential([
    Conv2D(26, (3, 3), input_shape=(64, 64, 3)),
    LeakyReLU(alpha=0.1),
    AveragePooling2D((2, 2)),
    Conv2D(32, (3, 3)),
    LeakyReLU(alpha=0.1),
    AveragePooling2D((2, 2)),
    Conv2D(64, (3, 3)),
    LeakyReLU(alpha=0.1),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(8, activation='relu'),
    Dropout(0.5),
    Dense(2, activation='softmax')
])

# model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Training the model using generators
history = model.fit(train_generator,
                    validation_data=valid_generator,
                    epochs=20,
                    callbacks=[early_stopping],
                    verbose=1)

# Clear memory
import gc
gc.collect()
     
Epoch 1/20
625/625 [==============================] - 104s 163ms/step - loss: 0.6194 - accuracy: 0.6654 - val_loss: 0.5139 - val_accuracy: 0.7714
Epoch 2/20
625/625 [==============================] - 99s 158ms/step - loss: 0.5221 - accuracy: 0.7650 - val_loss: 0.4238 - val_accuracy: 0.8174
Epoch 3/20
625/625 [==============================] - 101s 162ms/step - loss: 0.4620 - accuracy: 0.8012 - val_loss: 0.3888 - val_accuracy: 0.8360
Epoch 4/20
625/625 [==============================] - 104s 166ms/step - loss: 0.4208 - accuracy: 0.8202 - val_loss: 0.3423 - val_accuracy: 0.8552
Epoch 5/20
625/625 [==============================] - 100s 161ms/step - loss: 0.3841 - accuracy: 0.8437 - val_loss: 0.3143 - val_accuracy: 0.8790
Epoch 6/20
625/625 [==============================] - 102s 163ms/step - loss: 0.3656 - accuracy: 0.8501 - val_loss: 0.3164 - val_accuracy: 0.8676
Epoch 7/20
625/625 [==============================] - 100s 159ms/step - loss: 0.3344 - accuracy: 0.8683 - val_loss: 0.3181 - val_accuracy: 0.8708
Epoch 8/20
625/625 [==============================] - 101s 162ms/step - loss: 0.3124 - accuracy: 0.8774 - val_loss: 0.2878 - val_accuracy: 0.8852
Epoch 9/20
625/625 [==============================] - 111s 177ms/step - loss: 0.2946 - accuracy: 0.8860 - val_loss: 0.4152 - val_accuracy: 0.8012
Epoch 10/20
625/625 [==============================] - 99s 158ms/step - loss: 0.2772 - accuracy: 0.8948 - val_loss: 0.2929 - val_accuracy: 0.8834
Epoch 11/20
625/625 [==============================] - 100s 161ms/step - loss: 0.2584 - accuracy: 0.9014 - val_loss: 0.3200 - val_accuracy: 0.8576
Epoch 12/20
625/625 [==============================] - 101s 162ms/step - loss: 0.2403 - accuracy: 0.9090 - val_loss: 0.3299 - val_accuracy: 0.8726
Epoch 13/20
625/625 [==============================] - 100s 160ms/step - loss: 0.2264 - accuracy: 0.9165 - val_loss: 0.3468 - val_accuracy: 0.8788
21

# log loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.xlabel('Epochs')
plt.ylabel('Log Loss')
plt.legend()
plt.title('Log Loss over Epochs')
plt.show()

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test loss: {test_loss}')
print(f'Test accuracy: {test_accuracy}')
     

157/157 [==============================] - 9s 60ms/step - loss: 0.2837 - accuracy: 0.8864
Test loss: 0.2837495803833008
Test accuracy: 0.8863999843597412

#image preprocessing functions
def preprocess_image(image_path, target_size=(64, 64)):
    image = imread(image_path)
    image_resized = resize(image, target_size, anti_aliasing=True)
    image_normalized = image_normalization(image_resized)
    image_whitened = image_whitening(image_normalized)
    return np.expand_dims(image_whitened, axis=0)  # Add batch dimension

#function to classify an image
def classify_image(image_path):
    preprocessed_image = preprocess_image(image_path)
    prediction = model.predict(preprocessed_image)
    class_idx = np.argmax(prediction, axis=1)[0]
    class_labels = {0: 'FAKE', 1: 'REAL'}
    return class_labels[class_idx], prediction[0][class_idx]

# Example
image_path = '/content/Screenshot 2024-06-15 143904.jpg'
label, confidence = classify_image(image_path)
print(f"The image is classified as {label} with confidence {confidence:.2f}")

     
1/1 [==============================] - 0s 139ms/step
The image is classified as FAKE with confidence 0.91

tf.keras.models.save_model(model,'my_model2.hdf5')
     
<ipython-input-14-07e813155dae>:1: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  tf.keras.models.save_model(model,'my_model2.hdf5')

# Load your trained model
model = load_model('/content/my_model2.hdf5')

# image preprocessing functions
def preprocess_image(image, target_size=(64, 64)):
    img_resized = resize(image, target_size, anti_aliasing=True)
    img_normalized = image_normalization(img_resized)
    img_whitened = image_whitening(img_normalized)
    return np.expand_dims(img_whitened, axis=0)

# function to classify an image
def classify_image(image):
    preprocessed_image = preprocess_image(image)
    prediction = model.predict(preprocessed_image)
    class_idx = np.argmax(prediction, axis=1)[0]
    class_labels = {0: 'FAKE', 1: 'REAL'}
    return class_labels[class_idx], float(prediction[0][class_idx])

# Gradio interface
interface = gr.Interface(
    fn=classify_image,
    inputs=gr.Image(type="numpy", label="Upload Image"),
    outputs=[gr.Textbox(label="Class"), gr.Number(label="Confidence")],
    title="Image Classification",
    description="Upload an image to classify it as REAL or FAKE."
)

interface.launch()

     
Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).

Colab notebook detected. To show errors in colab notebook, set debug=True in launch()
Running on public URL: https://9c322b3164cdf84e30.gradio.live

This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)
OUR OTHER ATTEMPTS

Training using ResNet50. (rejected due to low accuracy)

# #ResNet50
# from tensorflow.keras.applications import ResNet50
# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout,Reshape
# from tensorflow.keras.models import Model
# from tensorflow.keras.callbacks import EarlyStopping

# # Load the ResNet50 model, excluding the top layers
# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# # Add custom layers on top of ResNet50
# x = base_model.output
# x = GlobalAveragePooling2D()(x)
# cnn_model = Sequential([
#     Conv2D(16, (3, 3), input_shape=(64, 64, 3)),
#     LeakyReLU(alpha=0.1),
#     AveragePooling2D((2, 2)),
#     Conv2D(32, (3, 3)),
#     LeakyReLU(alpha=0.1),
#     AveragePooling2D((2, 2)),
#     Conv2D(64, (3, 3)),
#     LeakyReLU(alpha=0.1),
#     MaxPooling2D((2, 2)),
#     Conv2D(128, (3, 3), activation='relu'),
#     MaxPooling2D((2, 2)),
#     Flatten(),
#     Dense(128, activation='relu'),
#     Dropout(0.5),
#     Dense(2, activation='softmax')
# ])
# x = Flatten()(x)
# x = Dense(64*64*3, activation='relu')(x)  # Reshape to be compatible with the custom CNN input
# x = Reshape((64, 64, 3))(x)
# output = cnn_model(x)

# # Define the model
# model = Model(inputs=base_model.input, outputs=output)

# # Freeze the layers of the base model
# for layer in base_model.layers:
#     layer.trainable = False

# # Compile the model
# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# # Define early stopping callback
# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# # Train the model using generators
# history = model.fit(train_generator,
#                     validation_data=valid_generator,
#                     epochs=20,
#                     callbacks=[early_stopping],
#                     verbose=1)

# # Clear memory
# import gc
# gc.collect()
     
Epoch 1/20
625/625 [==============================] - 31s 40ms/step - loss: 0.5375 - accuracy: 0.7330 - val_loss: 0.5113 - val_accuracy: 0.7456
Epoch 2/20
625/625 [==============================] - 24s 38ms/step - loss: 0.4773 - accuracy: 0.7719 - val_loss: 0.4809 - val_accuracy: 0.7766
Epoch 3/20
625/625 [==============================] - 24s 39ms/step - loss: 0.4525 - accuracy: 0.7883 - val_loss: 0.4662 - val_accuracy: 0.7718
Epoch 4/20
625/625 [==============================] - 25s 40ms/step - loss: 0.4317 - accuracy: 0.8015 - val_loss: 0.4686 - val_accuracy: 0.7848
Epoch 5/20
625/625 [==============================] - 26s 42ms/step - loss: 0.4177 - accuracy: 0.8100 - val_loss: 0.4474 - val_accuracy: 0.7900
Epoch 6/20
625/625 [==============================] - 24s 38ms/step - loss: 0.4015 - accuracy: 0.8185 - val_loss: 0.4513 - val_accuracy: 0.7924
Epoch 7/20
625/625 [==============================] - 25s 41ms/step - loss: 0.3818 - accuracy: 0.8290 - val_loss: 0.4552 - val_accuracy: 0.7822
Epoch 8/20
625/625 [==============================] - 24s 38ms/step - loss: 0.3618 - accuracy: 0.8400 - val_loss: 0.4690 - val_accuracy: 0.7844
Epoch 9/20
625/625 [==============================] - 25s 40ms/step - loss: 0.3437 - accuracy: 0.8501 - val_loss: 0.4908 - val_accuracy: 0.7714
Epoch 10/20
625/625 [==============================] - 24s 39ms/step - loss: 0.3223 - accuracy: 0.8612 - val_loss: 0.5313 - val_accuracy: 0.7650
19634

# # Plot log loss
# plt.plot(history.history['loss'], label='train loss')
# plt.plot(history.history['val_loss'], label='val loss')
# plt.xlabel('Epochs')
# plt.ylabel('Log Loss')
# plt.legend()
# plt.title('Log Loss over Epochs')
# plt.show()

# # Evaluate the model on the test data
# test_loss, test_accuracy = model.evaluate(test_generator)
# print(f'Test loss: {test_loss}')
# print(f'Test accuracy: {test_accuracy}')

     

157/157 [==============================] - 4s 25ms/step - loss: 0.4692 - accuracy: 0.7786
Test loss: 0.469226598739624
Test accuracy: 0.7785999774932861
Training using Ensemble on CNN. (rejected due to very high computation time)

# import os
# import shutil
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from skimage.io import imread
# from skimage.transform import resize
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping
# import random
# import matplotlib.pyplot as plt

# # Function to create a directory if it doesn't exist
# def create_directory(directory_path):
#     if not os.path.exists(directory_path):
#         os.makedirs(directory_path)

# # Function to sample and copy images from source to destination
# def sample_and_copy_images(src_directory, dest_directory, sample_fraction=0.2):
#     create_directory(dest_directory)
#     for sub_dir in ['FAKE', 'REAL']:
#         src_path = os.path.join(src_directory, sub_dir)
#         dest_path = os.path.join(dest_directory, sub_dir)
#         create_directory(dest_path)

#         all_files = os.listdir(src_path)
#         num_samples = int(len(all_files) * sample_fraction)
#         sampled_files = random.sample(all_files, num_samples)

#         for file_name in sampled_files:
#             shutil.copy(os.path.join(src_path, file_name), os.path.join(dest_path, file_name))

# # Directories
# src_train_directory = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train'
# src_test_directory = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test'
# dest_directory = '/kaggle/working/sampled_data'

# # Sample and copy images for initial training
# sample_and_copy_images(src_train_directory, os.path.join(dest_directory, 'initial_train'), sample_fraction=0.2)
# sample_and_copy_images(src_test_directory, os.path.join(dest_directory, 'test'), sample_fraction=0.2)

# # Load image paths and labels
# def load_image_paths_and_labels(directory):
#     data = []
#     labels = []
#     for label, sub_dir in enumerate(['FAKE', 'REAL']):
#         path = os.path.join(directory, sub_dir)
#         for file_name in os.listdir(path):
#             file_path = os.path.join(path, file_name)
#             data.append(file_path)
#             labels.append('FAKE' if label == 0 else 'REAL')
#     return data, labels

# train_paths, train_labels = load_image_paths_and_labels(os.path.join(dest_directory, 'initial_train'))

# # Data generators with data augmentation
# datagen = ImageDataGenerator(
#     preprocessing_function=lambda x: (x / 255.0 - np.mean(x / 255.0)) / np.std(x / 255.0),
#     rotation_range=20,
#     width_shift_range=0.2,
#     height_shift_range=0.2,
#     shear_range=0.2,
#     zoom_range=0.2,
#     horizontal_flip=True,
#     fill_mode='nearest'
# )

# def create_model():
#     model = Sequential([
#         Conv2D(32, (3, 3), input_shape=(64, 64, 3)),
#         LeakyReLU(alpha=0.1),
#         AveragePooling2D((2, 2)),
#         Conv2D(64, (3, 3)),
#         LeakyReLU(alpha=0.1),
#         AveragePooling2D((2, 2)),
#         Conv2D(128, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(2, activation='softmax')
#     ])
#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
#     return model

# def train_and_boost(train_paths, train_labels, n_models=5):
#     trained_models = []
#     for i in range(n_models):
#         # Split into training and validation sets
#         X_train, X_valid, y_train, y_valid = train_test_split(train_paths, train_labels, test_size=0.2, random_state=42, stratify=train_labels)

#         train_generator = datagen.flow_from_dataframe(
#             dataframe=pd.DataFrame({'filename': X_train, 'class': y_train}),
#             x_col='filename',
#             y_col='class',
#             target_size=(64, 64),
#             batch_size=32,
#             class_mode='categorical',
#             shuffle=True
#         )

#         valid_generator = datagen.flow_from_dataframe(
#             dataframe=pd.DataFrame({'filename': X_valid, 'class': y_valid}),
#             x_col='filename',
#             y_col='class',
#             target_size=(64, 64),
#             batch_size=32,
#             class_mode='categorical',
#             shuffle=True
#         )

#         # Train the model
#         model = create_model()
#         early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
#         history = model.fit(train_generator,
#                             validation_data=valid_generator,
#                             epochs=50,
#                             callbacks=[early_stopping],
#                             verbose=1)

#         # Identify misclassified images and boost
#         misclassified_indices = []
#         predictions = model.predict(valid_generator)
#         pred_labels = np.argmax(predictions, axis=1)
#         true_labels = valid_generator.classes

#         for idx, (true, pred) in enumerate(zip(true_labels, pred_labels)):
#             if true != pred:
#                 misclassified_indices.append(idx)

#         misclassified_paths = [X_valid[idx] for idx in misclassified_indices]
#         misclassified_labels = [y_valid[idx] for idx in misclassified_indices]

#         train_paths.extend(misclassified_paths)
#         train_labels.extend(misclassified_labels)

#         trained_models.append(model)

#     return trained_models

# # Train models using boosting
# models = train_and_boost(train_paths, train_labels, n_models=5)

# # Make predictions using ensemble voting
# test_paths, test_labels = load_image_paths_and_labels(os.path.join(dest_directory, 'test'))

# test_generator = datagen.flow_from_dataframe(
#     dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels}),
#     x_col='filename',
#     y_col='class',
#     target_size=(64, 64),
#     batch_size=32,
#     class_mode='categorical',
#     shuffle=False
# )

# predictions = [model.predict(test_generator) for model in models]
# avg_predictions = np.mean(predictions, axis=0)
# final_predictions = np.argmax(avg_predictions, axis=1)

# # Convert class indices back to labels
# class_indices = {v: k for k, v in test_generator.class_indices.items()}
# final_labels = [class_indices[pred] for pred in final_predictions]

# # Evaluate the ensemble model
# test_true_labels = test_generator.classes
# accuracy = np.mean(final_labels == test_true_labels)
# print(f'Ensemble model accuracy: {accuracy}')

     
We also used t-SNE to visualize data distribution so that we could potentially use KNN. (Rejected due to no grouping of data points.)


     
